<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <title>It's Only a Morpheme if it Comes from the Lexical Region of
            Meaning</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="stylesheet"
            href="https://cdnjs.cloudflare.com/ajax/libs/tufte-css/1.8.0/tufte.min.css" />
        <link rel="stylesheet"
            href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/katex.min.css">
        <script
            src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/katex.min.js"></script>
        <script
            src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/contrib/auto-render.min.js"></script>
        <style>
            h1.title {
                width: 70%;
            }
            h2 {
                margin-top: 0;
            }
            table {
                border-collapse: collapse;
                margin: 1rem 0;
                width: 100%;
            }
            table th, table td {
                border: 1px solid #ccc;
                padding: 0.5rem;
                text-align: left;
            }
        </style>
    </head>

    <body>
        <article>
            <h1 class="title">It's Only a Morpheme if it Comes from the Lexical
                Region of Meaning</h1>
            <p class="subtitle">11-424/11-824 Course Project</p>

            <section class="abstract">
                <p>In this project, I explored several approaches to
                    morphological segmentation for two low-resource languages. I
                    started with a traditional information-theoretic approach
                    using conditional entropy, then experimented with more
                    modern neural approaches like transformers and autoencoders.
                    Ultimately, I found that a neural scoring method that
                    combines the principled aspects of information theory with
                    the flexibility of neural networks performed best,
                    showing that integrating ideas from linguistic theory and
                    modern machine learning can be effective.</p>
            </section>

            <section>
                <h2>Introduction: What Are Morphemes and Why Do We Care?</h2>

                <p>Morphemes are the smallest meaningful units of language that
                    carry semantic or grammatical meaning. While modern NLP
                    systems commonly use subword tokenization methods like BPE
                    (Byte Pair Encoding) that are also based on
                    information-theoretic principles, morphemes differ in that
                    they're defined by linguistic meaning rather than
                    statistical frequency. For example, the English word
                    "invincible" consists of three morphemes: "in-" (meaning
                    "not"), "vinc" (the root meaning "conquer"), and "-ible"
                    (meaning
                    "able to be").<label for="sn-morpheme"
                        class="margin-toggle sidenote-number"></label><input
                        type="checkbox" id="sn-bpe"
                        class="margin-toggle" /><span class="sidenote">This is
                        just
                        my best guess at how the word would break down. Actual
                        linguists may disagree!</span>
                    Each of these pieces contributes to the
                    overall meaning of the word in a systematic way.</p>

                <p>Morphological analysis is fundamental to understanding
                    linguistic structure across languages. Languages can be
                    categorized along a spectrum based on their morphological
                    typology:<label for="sn-typology"
                        class="margin-toggle sidenote-number"></label><input
                        type="checkbox" id="sn-typology"
                        class="margin-toggle" /><span class="sidenote">Comrie,
                        Bernard.
                        "Language Universals and Linguistic Typology: Syntax and Morphology."
                        University of Chicago Press, 1989.</span></p>

                <ul>
                    <li><strong>Isolating languages</strong> (like Mandarin
                        Chinese) have words that tend to consist of single
                        morphemes, with limited affixation.</li>
                    <li><strong>Fusional languages</strong> (like Latin,
                        Russian, or Arabic) have morphemes that often fuse
                        multiple grammatical functions. For instance, a single
                        suffix might simultaneously mark case, number, and
                        gender.</li>
                    <li><strong>Agglutinative languages</strong> (like Turkish,
                        Finnish, or Japanese) string together morphemes in long
                        chains, with each morpheme typically representing one
                        grammatical function.</li>
                    <li><strong>Polysynthetic languages</strong> (like many
                        indigenous languages of the Americas, including
                        Inuktitut and Mohawk) can pack what would be an entire
                        sentence in English into a single complex word.</li>
                </ul>

                <p>The languages we're studying in this project - Rar치muri and
                    Shipibo-Konibo - fall toward the agglutinative and
                    polysynthetic end of this spectrum, making morphological
                    segmentation both challenging and particularly important for
                    understanding their structure.</p>

                <p>So why is morphological segmentation important? First,
                    morphology is critical for understanding how words are
                    formed across languages. Different languages build words in
                    different ways, and morphological analysis helps us
                    understand these patterns. From a computational perspective,
                    a proper morphological analysis can help with tasks like
                    machine translation, information retrieval, and text
                    generation, especially for morphologically rich languages
                    where a single word can encode what might be an entire
                    phrase in English.<label for="sn-1"
                        class="margin-toggle sidenote-number"></label><input
                        type="checkbox" id="sn-1" class="margin-toggle" /><span
                        class="sidenote">Bender, Emily M.
                        "Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Morphology and Syntax."
                        Morgan & Claypool Publishers, 2013.</span></p>

                <p>The practical applications of accurate morphological
                    segmentation are particularly salient for low-resource
                    languages. When working with limited data, capturing the
                    grammatical regularities encoded in morphology can
                    improve model performance on downstream tasks. For instance,
                    in machine translation, recognizing that
                    Turkish <em>evlerimden</em> decomposes into
                    <em>ev-ler-im-den</em> ("from my houses") allows a model to
                    generalize across all words with similar affixes, rather
                    than treating each word form as completely
                    independent.<label for="sn-mt-morph"
                        class="margin-toggle sidenote-number"></label><input
                        type="checkbox" id="sn-mt-morph"
                        class="margin-toggle" /><span class="sidenote">Ataman,
                        Duygu, et al.
                        "Unsupervised Neural Machine Translation with Unsupervised Morphological Segmentation."
                        EMNLP, 2019.</span></p>

                <p>In this project, I aimed to develop tokenizers that segment
                    text from two indigenous languages into morphemes, trying to
                    match gold standard segmentations created by linguists. The
                    languages are:</p>

                <ul>
                    <li><strong>Rar치muri (Tahumara) [tar]:</strong> an
                        indigenous language from Northern Mexico in the
                        Uto-Aztecan family. It has approximately 70,000 speakers
                        primarily in the state of Chihuahua. Rar치muri features
                        complex verbal morphology with numerous suffixes that
                        can be stacked to create intricate meanings.<label
                            for="sn-raramuri"
                            class="margin-toggle sidenote-number"></label><input
                            type="checkbox" id="sn-raramuri"
                            class="margin-toggle" /><span
                            class="sidenote">Caballero, Gabriela.
                            "Choguita Rar치muri (Tarahumara) Phonology and Morphology."
                            University of California, Berkeley,
                            2008.</span></li>
                    <li><strong>Shipibo-Konibo [shp]:</strong> a Panoan language
                        from the Peruvian Amazon with around 30,000 speakers.
                        It's characterized by its agglutinative structure and
                        extensive suffix system. The language has grammatical
                        features like ergative-absolutive alignment and a rich
                        system of evidential markers (indicating the source of
                        information).<label for="sn-shipibo"
                            class="margin-toggle sidenote-number"></label><input
                            type="checkbox" id="sn-shipibo"
                            class="margin-toggle" /><span
                            class="sidenote">Valenzuela, Pilar M.
                            "Transitivity in Shipibo-Konibo Grammar." University
                            of Oregon, 2003.</span></li>
                </ul>

                <p>These languages are particularly interesting for
                    morphological segmentation because they exhibit rich
                    morphological structures that differ significantly from
                    better-studied languages like English. Working with these
                    low-resource languages also presents an opportunity to
                    develop methods that could be valuable for many other
                    understudied languages around the world.</p>
            </section>

            <section>
                <h2>The Dataset and Task</h2>

                <p>The dataset consists of words and their morphological
                    segmentations for both languages. For example:</p>

                <pre>
# Shipibo-Konibo examples
Source: yoyoaibata     Target: yoyoa ibat a
Source: kotsatax       Target: kotsat ax
Source: bokasai        Target: bo kas ai

# Rar치muri examples  
Source: p치a            Target: p치a
Source: poch칤tisi      Target: poch칤 ti si
Source: kon치ri         Target: ko n치ri</pre>

                <p>My task was to develop a system that, given an unsegmented
                    word, produces a segmentation that matches the linguist's
                    gold segmentation as closely as possible. The evaluation
                    metric is precision/recall/F1 on a "bag of tokens" (rather
                    than exact segmentation boundaries) due to allomorphy in the
                    data.</p>

                <p>Allomorphy is when a morpheme has different
                    forms (allomorphs) depending on its context. For example, in
                    English, the plural morpheme has allomorphs like "-s"
                    (cats), "-es" (boxes), and "-en" (oxen). In our dataset,
                    some morphemes don't concatenate perfectly, meaning there
                    can be additional characters at morpheme boundaries.</p>

                <p>To deal with this, I implemented a preprocessing step that
                    "cleans" target segmentations using Levenshtein distance to
                    align segments with the source word. This ensures that our
                    model isn't penalized for segmentation errors due to
                    allomorphy.</p>

                <pre>
def clean_target_segmentation(src, tgt_line):
    tgt_segs = tgt_line.split()
    k = len(tgt_segs)
    n = len(src)
    dp = [[float('inf')] * (k+1) for _ in range(n+1)]
    bp = [[-1] * (k+1) for _ in range(n+1)]
    dp[0][0] = 0
    for i in range(n+1):
        for j in range(k):
            if dp[i][j] < float('inf'):
                for l in range(1, n - i + 1):
                    segment = src[i:i+l]
                    cost = levenshtein_distance(segment, tgt_segs[j])
                    if dp[i][j] + cost < dp[i+l][j+1]:
                        dp[i+l][j+1] = dp[i][j] + cost
                        bp[i+l][j+1] = l
    # Backtracking to get the best alignment
    if dp[n][k] == float('inf'):
        return tgt_segs
    segments = []
    i, j = n, k
    while j > 0:
        l = bp[i][j]
        segments.append(src[i-l:i])
        i -= l
        j -= 1
    return list(reversed(segments))</pre>

                <p>The baselines provided for comparison were:</p>

                <table>
                    <tr>
                        <th>Method</th>
                        <th>Shipibo-Konibo (F1)</th>
                        <th>Rar치muri (F1)</th>
                    </tr>
                    <tr>
                        <td>Morfessor</td>
                        <td>0.3207</td>
                        <td>0.2959</td>
                    </tr>
                    <tr>
                        <td>BPE</td>
                        <td>0.2682</td>
                        <td>0.4213</td>
                    </tr>
                    <tr>
                        <td>Unigram</td>
                        <td>0.2616</td>
                        <td>0.3369</td>
                    </tr>
                </table>

                <p>My goal was simply to improve on these baselines with my own
                    approach.</p>
            </section>

            <section>
                <h2>First Attempt: Information-Theoretic Approach</h2>

                <p>I started with an information-theoretic approach based on
                    conditional entropy. The intuition is that morpheme
                    boundaries often correspond to points of high uncertainty or
                    "surprisal" in character sequences. When reading a word
                    character by character, the first character of a new
                    morpheme is typically less predictable than one that
                    continues the current morpheme. This approach is
                    conceptually
                    related to methods like BPE, but it focuses on identifying
                    boundaries based on
                    predictability rather than mere frequency.</p>

                <p>This makes some intuitive sense. Since morphemes are by
                    definition minimum meaningful units, they should
                    exhibit high internal coherence
                    (low surprisal within a morpheme) and lower coherence at
                    boundaries (high surprisal across morpheme
                    boundaries).<label
                        for="sn-3"
                        class="margin-toggle sidenote-number"></label><input
                        type="checkbox" id="sn-3" class="margin-toggle" /><span
                        class="sidenote">Goldsmith, John.
                        "Unsupervised Learning of the Morphology of a Natural Language."
                        Computational Linguistics, 2001.</span></p>

                <p>This approach is related to the Minimum Description
                    Length (MDL) principle, which suggests that an efficient
                    linguistic
                    representation balances the size of the grammar with the
                    size of the encoded data.<label for="sn-mdl"
                        class="margin-toggle sidenote-number"></label><input
                        type="checkbox" id="sn-mdl"
                        class="margin-toggle" /><span class="sidenote">Rissanen,
                        Jorma. "Modeling by Shortest Data Description."
                        Automatica, 1978.</span></p>

                <p>Formally, we can calculate the surprisal of a character
                    given its prefix using:</p>

                <p>\[
                    S = -\log P(x \mid \text{prefix})
                    \]</p>

                <p>where the probability is estimated using an n-gram model
                    built from the training corpus. When this surprisal exceeds
                    a preset threshold, a morpheme boundary is placed. The idea
                    is that the transition between morphemes will be marked by
                    higher surprisal values.</p>

                <pre>
class NGramModel:
    # character-level n-gram model with conditional entropy segmentation
    
    def __init__(self, smoothing_constant=1.0, surprisal_threshold=3.0):
        self.smoothing_constant = smoothing_constant
        self.ngram_counts = defaultdict(lambda: defaultdict(int))
        self.vocabulary = set()
        self.surprisal_threshold = surprisal_threshold

    def fit(self, words):
        for word in words:
            word += '<END>'
            self.vocabulary.update(word)
            for i in range(len(word)):
                for j in range(1, i + 2):
                    prefix = word[i - j + 1:i]
                    char = word[i]
                    self.ngram_counts[prefix][char] += 1

    def next_letter_surprisal(self, prefix, char):
        count = self.ngram_counts[prefix][char]
        total = sum(self.ngram_counts[prefix].values())
        vocab_size = len(self.vocabulary)
        
        # surprisal = -log P(char | prefix)
        probability = (count + self.smoothing_constant) / (total + self.smoothing_constant * vocab_size)
        
        return -math.log(probability)

    def segment_word(self, word):
        segments = []
        start = 0
        for i in range(1, len(word)):
            prefix = word[start:i]
            next_char = word[i]
            surprisal = self.next_letter_surprisal(prefix, next_char)
            if surprisal > self.surprisal_threshold:
                segments.append(word[start:i])
                start = i
        segments.append(word[start:])
        return " ".join(segments)</pre>
                <p>I experimented with different surprisal thresholds to
                    identify the best values for each language. The best
                    thresholds turned out to be around 3.06 for Shipibo-Konibo
                    (F1 = 0.3326) and 2.86 for Rar치muri (F1 = 0.2680). While
                    this approach improved upon some baselines for one language,
                    it did not for the other.</p>

                <figure>
                    <img src="images/surprisal_thresholds.png"
                        alt="F1 score vs. surprisal threshold for Shipibo-Konibo and Rar치muri"
                        loading="lazy">
                    <span class="marginnote">Figure 1: F1 score as a function of
                        surprisal threshold. Peaks indicate the selected
                        thresholds for each language.</span>
                </figure>

                <p>While elegant in theory, this approach relies solely on
                    character-level statistics and does not incorporate
                    linguistic patterns or semantic information that modern
                    neural methods might capture. Nonetheless, it serves as a
                    useful starting point for thinking about the problem.</p>
            </section>

            <section>
                <h2>Second Attempt: Transformer-Based Approach</h2>

                <p>Next, I experimented with a more modern neural network
                    approach by framing morphological segmentation as a sequence
                    labeling task. In this setting, each character is labeled as
                    either the start of a morpheme (1) or a continuation (0),
                    and I implemented this using a transformer
                    architecture.</p>

                <p>I implemented a character-level transformer model with
                    positional encoding:</p>

                <pre>
class MorphemeTransformer(nn.Module):
    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, dropout, max_len):
        super(MorphemeTransformer, self).__init__()

        # Config
        self.embed_dim = embed_dim
        self.pad_idx = 0  # padding token index

        # Token and position encodings
        self.embedding = nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=embed_dim,
            padding_idx=self.pad_idx,
        )
        self.pos_encoder = PositionalEncoding(
            d_model=embed_dim,
            dropout=dropout,
            max_len=max_len,
        )

        # Transformer encoder stack (expects input as (seq_len, batch, embed_dim))
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=embed_dim,
            nhead=num_heads,
            dropout=dropout,
        )
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer=encoder_layer,
            num_layers=num_layers,
        )

        # Per-character binary classifier: 0 = continue, 1 = start
        self.fc = nn.Linear(embed_dim, 2)

    def forward(self, src: torch.Tensor) -> torch.Tensor:
        """
        Args:
            src: LongTensor of shape (batch, seq_len) with character indices.
        Returns:
            FloatTensor of shape (batch, seq_len, 2) with per-character logits.
        """
        # Embed tokens and add positional encodings
        emb = self.embedding(src)                  # (batch, seq_len, embed_dim)
        emb = self.pos_encoder(emb)                # (batch, seq_len, embed_dim)

        # TransformerEncoder expects (seq_len, batch, embed_dim)
        enc_in = emb.transpose(0, 1)               # (seq_len, batch, embed_dim)
        enc_out = self.transformer_encoder(enc_in) # (seq_len, batch, embed_dim)

        # Back to (batch, seq_len, embed_dim) and classify
        enc_out = enc_out.transpose(0, 1)          # (batch, seq_len, embed_dim)
        logits = self.fc(enc_out)                  # (batch, seq_len, 2)
        return logits</pre>

                <p>I trained the model using cross-entropy loss on the training
                    set and evaluated it on the development set. The
                    transformer-based approach performed better than the
                    baselines in terms of F1 score:</p>

                <p>Shipibo-Konibo dev F1 score: 0.7843<br>
                    Rar치muri dev F1 score: 0.8357</p>

                <p>After training on the combined train and development sets,
                    the test F1 scores were:</p>

                <p>Shipibo-Konibo test F1 score: 0.6435<br>
                    Rar치muri test F1 score: 0.4892</p>

                <p>While this approach improved over the baselines, it did not
                    provide much insight into the linguistic factors behind the
                    segmentation decisions, which led me to explore more
                    interpretable methods.</p>
            </section>

            <section>
                <h2>Third Attempt: Autoencoder Approach</h2>

                <p>Next, I explored an autoencoder-based approach. The idea was
                    to learn a compressed representation of the input data where
                    the latent space captures morpheme boundaries. This approach
                    is connected to the information-theoretic view that
                    morphemes are compressed representations of meaning.<label
                        for="sn-4"
                        class="margin-toggle sidenote-number"></label><input
                        type="checkbox" id="sn-4" class="margin-toggle" /><span
                        class="sidenote">Cotterell, Ryan, et al.
                        "Morphological Segmentation Inside-Out." EMNLP,
                        2016.</span></p>

                <p>However, autoencoders inherently have a continuous latent
                    space, while morpheme boundaries are discrete. To bridge
                    this gap, I used the Gumbel-Softmax trick, which allows
                    sampling from a categorical distribution in a differentiable
                    way by adding Gumbel noise and applying a softmax with a
                    temperature parameter.<label for="sn-gumbel"
                        class="margin-toggle sidenote-number"></label><input
                        type="checkbox" id="sn-gumbel"
                        class="margin-toggle" /><span class="sidenote">Jang,
                        Eric, et al.
                        "Categorical Reparameterization with Gumbel-Softmax."
                        ICLR, 2017.</span></p>

                <!-- <figure>
                    <img src="/api/placeholder/800/400"
                        alt="Autoencoder Architecture with Gumbel-Softmax">
                    <figcaption>Figure 2: Autoencoder architecture with
                        Gumbel-Softmax for discrete latent
                        representation</figcaption>
                </figure> -->

                <p>The autoencoder model architecture consists of:</p>

                <ol>
                    <li>A character embedding layer that converts input indices
                        to dense vectors</li>
                    <li>An LSTM encoder that produces hidden states for each
                        character</li>
                    <li>A linear projection layer that maps these hidden states
                        to logits for binary decisions (morpheme boundary or
                        not)</li>
                    <li>A Gumbel-Softmax layer that converts these logits to
                        differentiable discrete samples</li>
                    <li>A decoder LSTM that uses both the embeddings and the
                        discrete latent variables to reconstruct the input</li>
                </ol>

                <p>The implementation looks like this:</p>
                <pre><code class="language-python">
class MorphAutoencoder(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, pad_idx):
        super(MorphAutoencoder, self).__init__()

        # Config
        self.embed_dim = embed_dim
        self.hidden_dim = hidden_dim
        self.pad_idx = pad_idx

        # Encoder
        self.embedding = nn.Embedding(
            num_embeddings=vocab_size,
            embedding_dim=embed_dim,
            padding_idx=self.pad_idx,
        )
        self.encoder = nn.LSTM(
            input_size=embed_dim,
            hidden_size=hidden_dim,
            batch_first=True,
        )

        # Boundary predictor (2 classes: 0 = continue, 1 = start)
        self.boundary_head = nn.Linear(hidden_dim, 2)

        # Decoder (condition on embeddings + boundary one-hot)
        self.decoder = nn.LSTM(
            input_size=embed_dim + 2,
            hidden_size=hidden_dim,
            batch_first=True,
        )
        self.output_head = nn.Linear(hidden_dim, vocab_size)

    def forward(self, src: torch.Tensor, tau: float = 1.0, hard: bool = True):
        """
        Args:
            src: LongTensor of shape (batch, seq_len) with character indices.
            tau: float Gumbel-Softmax temperature.
            hard: bool; if True, straight-through one-hot boundary samples.

        Returns:
            logits: FloatTensor of shape (batch, seq_len, vocab_size) reconstruction logits.
            boundary_logits: FloatTensor of shape (batch, seq_len, 2) raw boundary logits.
            boundary_onehot: FloatTensor of shape (batch, seq_len, 2) Gumbel-Softmax samples.
        """
        # Encode tokens
        emb = self.embedding(src)                           # (batch, seq_len, embed_dim)
        enc_out, _ = self.encoder(emb)                      # (batch, seq_len, hidden_dim)

        # Boundary logits and Gumbel-Softmax samples
        boundary_logits = self.boundary_head(enc_out)       # (batch, seq_len, 2)
        boundary_onehot = F.gumbel_softmax(
            boundary_logits, tau=tau, hard=hard, dim=-1
        )                                                   # (batch, seq_len, 2)

        # Decode (condition on boundaries)
        dec_in = torch.cat([emb, boundary_onehot], dim=-1)  # (batch, seq_len, embed_dim + 2)
        dec_out, _ = self.decoder(dec_in)                   # (batch, seq_len, hidden_dim)
        logits = self.output_head(dec_out)                  # (batch, seq_len, vocab_size)

        return logits, boundary_logits, boundary_onehot
</code></pre>

                <p>For training, I used two loss components:</p>

                <p>\[
                    \mathcal{L}_{\text{recon}} = -\frac{1}{N}\sum_{i=1}^{N}
                    \mathbb{1}_{x_i \neq 0} \sum_{c=1}^{C} y_{i,c}
                    \log(\hat{y}_{i,c})
                    \]</p>

                <p>\[
                    \mathcal{L}_{\text{seg}} = -\frac{1}{M}\sum_{j=1}^{M}
                    \sum_{k=0}^{1} z_{j,k} \log(p_{j,k})
                    \]</p>

                <p>\[
                    \mathcal{L}_{\text{total}} = \lambda_{\text{recon}} \cdot
                    \mathcal{L}_{\text{recon}} + \lambda_{\text{seg}} \cdot
                    \mathcal{L}_{\text{seg}}
                    \]</p>

                <p>Weighing these losses together is important. Too much
                    emphasis on reconstruction encourages trivial copying of the
                    input, while too much emphasis on segmentation degrades
                    reconstruction quality.</p>

                <p>I used Optuna to tune the relative weights (lambda_seg and
                    lambda_recon) and found optimal values of lambda_seg=0.559
                    and lambda_recon=3.542 after 10 trials. This suggests that
                    the model benefits from a stronger emphasis on
                    reconstruction relative to segmentation.</p>

                <p>On the development set, the results were:</p>

                <p>Shipibo-Konibo dev F1 score: 0.6957<br>
                    Rar치muri dev F1 score: 0.7346</p>

                <p>And on the test set:</p>

                <p>Shipibo-Konibo test F1 score: 0.7380<br>
                    Rar치muri test F1 score: 0.5024</p>

                <p>This approach improved over the transformer for
                    Shipibo-Konibo but was slightly lower for Rar치muri.</p>
            </section>

            <section>
                <h2>Final Approach: Neural Scoring for Segmentation</h2>

                <p>After reviewing the previous approaches, I combined the
                    information-theoretic ideas with neural networks via
                    differentiable programming. The goal was to use a neural
                    scoring function to assign a quality score to every possible
                    candidate segment in a word, and then use dynamic
                    programming to compute a global score for the
                    segmentation.</p>

                <p>This approach draws inspiration from structured prediction
                    methods.<label for="sn-5"
                        class="margin-toggle sidenote-number"></label><input
                        type="checkbox" id="sn-5" class="margin-toggle" /><span
                        class="sidenote">Mensch, Arthur, and Mathieu Blondel.
                        "Differentiable Dynamic Programming for Structured Prediction and Attention."
                        ICML, 2018.</span>
                    Instead of making independent decisions for each
                    character, it scores complete segmentations, acknowledging
                    that segmentation decisions are interdependent.</p>

                <p>The steps implemented were:</p>

                <ol>
                    <li><strong>Encoding:</strong> Use a character-level
                        bidirectional LSTM to obtain hidden representations
                        \(h_1, h_2, \dots, h_n\) for the input word.</li>
                    <li><strong>Scoring Segments:</strong> For any candidate
                        segment spanning positions \(i\) to \(j\), compute a
                        score \(s(i,j) = \text{MLP}(h_i \oplus h_{j-1})\), where
                        \(\oplus\) denotes vector concatenation. This score
                        reflects the plausibility of the substring being a valid
                        morpheme.</li>
                    <li><strong>Global Segmentation Score:</strong> Use a
                        differentiable dynamic programming algorithm to compute
                        the overall segmentation score and the partition
                        function \(Z\), where:
                        <p>\[
                            Z = \sum_{S} \exp(s(S))
                            \]</p>
                        <p>and
                            <p>\[
                                \log Z = \log\left(\sum_{S} \exp(s(S))\right)
                                \]</p>
                            <p>The dynamic programming recurrence is given by:
                                <p>\[
                                    \text{dp}[i] = \log\sum_{j=0}^{i-1}
                                    \exp(\text{dp}[j] +
                                    s(j,i))
                                    \]</p>
                            </p>
                        </li>
                        <li><strong>Loss Function:</strong> Define the loss as
                            \(\mathcal{L} = \log Z - s_{\text{gold}}\), which
                            encourages a higher global score for the correct
                            segmentation relative to all possible
                            segmentations.</li>
                    </ol>

                    <p>Here's the implementation of the Segmenter model:</p>

                    <pre>
class Segmenter(nn.Module):
    def __init__(self, vocab_size, emb_dim, hidden_dim, max_seg_len, char2idx, idx2char):
        super().__init__()
        self.pad_idx = char2idx['<PAD>']
        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=self.pad_idx)
        self.lstm = nn.LSTM(emb_dim, hidden_dim, bidirectional=True, batch_first=True)
        # scoring MLP
        self.mlp = nn.Sequential(nn.Linear(4*hidden_dim, 128),
                               nn.ReLU(), 
                               nn.Linear(128, 1))
        self.idx2char = idx2char

    # Score a segment from position i to j
    def segment_score(self, h, i, j):
        seg_rep = torch.cat([h[i], h[j-1]], dim=-1)
        return self.mlp(seg_rep).squeeze()
    
    # Compute loss using dynamic programming
    def dp_loss(self, h, gold_boundaries):
        L = h.size(0)
        dp = [None]*(L+1)
        dp[0] = torch.tensor(0.0, device=h.device)
        
        # Forward pass: compute log-partition function
        for i in range(1, L+1):
            candidates = []
            for j in range(0, i):
                score = self.segment_score(h, j, i)
                candidates.append(dp[j] + score)
            dp[i] = torch.logsumexp(torch.stack(candidates), dim=0)
        
        # Compute score of gold segmentation
        logZ = dp[L]
        gold_score = 0.0
        gold_boundaries_full = gold_boundaries[:]
        
        if gold_boundaries_full[-1] != L:
            gold_boundaries_full.append(L)
        
        for idx in range(1, len(gold_boundaries_full)):
            j = gold_boundaries_full[idx-1]
            i = gold_boundaries_full[idx]
            gold_score = gold_score + self.segment_score(h, j, i)
        
        # Return negative log-likelihood: log(Z) - score(gold)
        return logZ - gold_score

    # Viterbi decoding to find best segmentation
    def viterbi_decode(self, h):
        L = h.size(0)
        dp = [None]*(L+1)           # dp[i] stores the best score for the first i characters
        bp = [None]*(L+1)           # bp[i] stores the best boundary before the i-th character
        dp[0] = 0.0
        
        # dp[i] = max(dp[j] + score(j, i)) for 0 <= j < i
        for i in range(1, L+1):
            best_score = -float('inf')
            best_j = None
            for j in range(0, i):
                score = dp[j] + self.segment_score(h, j, i).item()
                if score > best_score:
                    best_score = score
                    best_j = j
            dp[i] = best_score
            bp[i] = best_j
        
        # Backtrack to recover the best segmentation
        boundaries = []
        i = L
        while i > 0:
            boundaries.append(bp[i])
            i = bp[i]
        boundaries = boundaries[::-1]
        boundaries.append(L)
        return boundaries

    # Forward pass
    def forward(self, src, labels=None):
        emb = self.emb(src)
        out, _ = self.lstm(emb)
        if labels is not None:
            loss = 0.0    
            for b in range(src.size(0)):
                l = (src[b] != self.pad_idx).sum().item()
                h = out[b, :l, :]
                gold_b = self.get_gold_boundaries(labels[b, :l])
                loss = loss + self.dp_loss(h, gold_b)
            return loss / src.size(0)
        else:
            preds = []
            for b in range(src.size(0)):
                l = (src[b] != self.pad_idx).sum().item()
                h = out[b, :l, :]
                preds.append(self.decode_word(h, src[b]))
            return preds</pre>

                    <p>This approach combines dynamic programming, which
                        enforces global consistency, with the representational
                        capabilities of neural networks. The entire pipeline is
                        differentiable, allowing for end-to-end training with
                        gradient descent.</p>

                    <figure>
                        <img src="/api/placeholder/800/400"
                            alt="Neural Segmentation with Dynamic Programming">
                        <figcaption>Figure 3: Neural segmentation using dynamic
                            programming to compute globally optimal
                            segmentations based on neural segment
                            scores.</figcaption>
                    </figure>

                    <p>After training for 25 epochs, the model achieved the
                        following:</p>

                    <p>Shipibo-Konibo dev F1 score: 0.7854<br>
                        Rar치muri dev F1 score: 0.7493</p>

                    <p>And on the test set:</p>

                    <p>Shipibo-Konibo test F1 score: 0.8126<br>
                        Rar치muri test F1 score: 0.6018</p>

                    <p>Among the approaches I tried, the neural scoring method
                        produced the highest scores.</p>
                </section>

                <section>
                    <h2>Conclusion and Final Results</h2>

                    <p>Here is a summary of the results across all the
                        approaches:</p>

                    <table>
                        <tr>
                            <th>Method</th>
                            <th>Shipibo-Konibo (F1)</th>
                            <th>Rar치muri (F1)</th>
                        </tr>
                        <tr>
                            <td>Morfessor (baseline)</td>
                            <td>0.3207</td>
                            <td>0.2959</td>
                        </tr>
                        <tr>
                            <td>BPE (baseline)</td>
                            <td>0.2682</td>
                            <td>0.4213</td>
                        </tr>
                        <tr>
                            <td>Unigram (baseline)</td>
                            <td>0.2616</td>
                            <td>0.3369</td>
                        </tr>
                        <tr>
                            <td>Conditional Entropy</td>
                            <td>0.3326</td>
                            <td>0.2680</td>
                        </tr>
                        <tr>
                            <td>Transformer</td>
                            <td>0.6435</td>
                            <td>0.4892</td>
                        </tr>
                        <tr>
                            <td>Autoencoder</td>
                            <td>0.7380</td>
                            <td>0.5024</td>
                        </tr>
                        <tr>
                            <td>Neural Scoring</td>
                            <td>0.8126</td>
                            <td>0.6018</td>
                        </tr>
                    </table>

                    <p>The results show a consistent improvement from the
                        traditional methods to neural approaches, with the
                        neural scoring method achieving the highest F1 scores.
                        Some key observations include:</p>

                    <ol>
                        <li><strong>Global consistency:</strong> Approaches that
                            consider the entire word for segmentation tend to
                            perform better than those making independent
                            decisions for each character.</li>
                        <li><strong>Linguistic inductive bias:</strong>
                            Incorporating the intuition that words consist of a
                            sequence of morphemes helps guide the model.</li>
                        <li><strong>Representational power:</strong> A
                            bidirectional LSTM encoder can capture complex
                            patterns that signal morpheme boundaries.</li>
                        <li><strong>End-to-end optimization:</strong> The
                            differentiable design enables direct optimization of
                            segmentation quality.</li>
                    </ol>

                    <p>Overall, I found the project to be a fun exploration of
                        both traditional and neural methods for morphological
                        segmentation. It's nice when a method that's based on
                        linguistic principles can outperform more data-driven
                        approaches. Future directions could include
                        incorporating morphophonological rules, exploring
                        multilingual models, semi-supervised learning,
                        hierarchical segmentation, and extending the methods to
                        other morphological types.</p>
                </section>
            </article>

            <script>
            document.addEventListener("DOMContentLoaded", function() {
              renderMathInElement(document.body, {
                delimiters: [
                  {left: "$$", right: "$$", display: true},
                  {left: "\\[", right: "\\]", display: true},
                  {left: "$", right: "$", display: false},
                  {left: "\\(", right: "\\)", display: false}
                ]
              });
            });
        </script>
        </body>
    </html>
