<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <title>It's Only a Morpheme if it Comes from the Lexical Region of
            Meaning</title>
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <link rel="stylesheet"
            href="https://cdnjs.cloudflare.com/ajax/libs/tufte-css/1.8.0/tufte.min.css" />
        <link rel="stylesheet"
            href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/katex.min.css">
        <script
            src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/katex.min.js"></script>
        <script
            src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.4/contrib/auto-render.min.js"></script>
        <style>
    /* body { 
      max-width: 55%;
      counter-reset: sidenote-counter; 
    }
    h1.title { margin-top: 2rem; margin-bottom: 1.5rem; }
    .subtitle { font-style: italic; margin-top: 0; }
    .author { margin-top: 1rem; margin-bottom: 1rem; font-family: et-book, Palatino, "Palatino Linotype", "Palatino LT STD", "Book Antiqua", Georgia, serif; }
    .date { margin-top: 1rem; margin-bottom: 1rem; font-family: et-book, Palatino, "Palatino Linotype", "Palatino LT STD", "Book Antiqua", Georgia, serif; } */
    code { font-size: 0.9rem; }
    pre { 
      background-color: #f7f7f7; 
      padding: 1em; 
      overflow-x: auto;
      font-size: 0.9rem;
      margin-top: 1rem;
      margin-bottom: 1rem;
    }
    table {
      border-collapse: collapse;
      margin: 1rem 0;
      width: 100%;
    }
    table th, table td {
      border: 1px solid #ccc;
      padding: 0.5rem;
      text-align: left;
    }
    table th {
      background-color: #f2f2f2;
    }
    figure {
      margin: 1.5rem 0;
      text-align: center;
    }
    figure img {
      max-width: 100%;
    }
    figcaption {
      margin-top: 0.5rem;
      font-size: 1.1rem;
      font-style: italic;
    }
    .katex { font-size: 1.1em; }
    .abstract {
      margin: 2rem 0;
      font-size: 1.1rem;
      line-height: 1.6;
    }
    .abstract p:first-of-type:before {
      content: "Abstract: ";
      font-weight: bold;
    }
    /* @media (max-width: 760px) {
      body { max-width: 90%; }
    } */
  </style>
    </head>

    <body>
        <article>
            <h1 class="title">It's Only a Morpheme if it Comes from the Lexical
                Region of Meaning</h1>
            <p class="subtitle">11-424/11-824 Course Project</p>

            <section class="abstract">
                <p>In this project, I explored several approaches to
                    morphological segmentation for two low-resource languages. I
                    started with a traditional information-theoretic approach
                    using conditional entropy, then experimented with more
                    modern neural approaches like transformers and autoencoders.
                    Ultimately, I found that a neural scoring method that
                    combines the principled aspects of information theory with
                    the flexibility of neural networks performed best,
                    demonstrating that sometimes the marriage of linguistic
                    theory and modern machine learning yields superior
                    results.</p>
            </section>

            <section>
                <h2>Introduction: What Are Morphemes and Why Do We Care?</h2>

                <p>Morphemes are the smallest meaningful units of language that
                    carry semantic or grammatical meaning. While modern NLP
                    systems commonly use subword tokenization methods like BPE
                    (Byte Pair Encoding) that are also based on
                    information-theoretic principles, morphemes differ in that
                    they're defined by linguistic meaning rather than
                    statistical frequency.<label for="sn-bpe"
                        class="margin-toggle sidenote-number"></label><input
                        type="checkbox" id="sn-bpe"
                        class="margin-toggle" /><span class="sidenote">Methods
                        like BPE and SentencePiece use statistical approaches to
                        identify frequent character sequences, creating
                        efficient vocabulary compression. While effective for
                        many NLP tasks, these statistically-derived units don't
                        always align with linguistically meaningful
                        units.</span> For example, the English word
                    "unbreakable" consists of three morphemes: "un-" (meaning
                    "not"), "break" (the root), and "-able" (meaning
                    "can be done"). Each of these pieces contributes to the
                    overall meaning of the word in a systematic way.</p>

                <p>Morphological analysis is fundamental to understanding
                    linguistic structure across languages. Languages can be
                    categorized along a spectrum based on their morphological
                    typology:<label for="sn-typology"
                        class="margin-toggle sidenote-number"></label><input
                        type="checkbox" id="sn-typology"
                        class="margin-toggle" /><span class="sidenote">Comrie,
                        Bernard.
                        "Language Universals and Linguistic Typology: Syntax and Morphology."
                        University of Chicago Press, 1989.</span></p>

                <ul>
                    <li><strong>Isolating languages</strong> (like Mandarin
                        Chinese) have words that tend to consist of single
                        morphemes, with limited affixation.</li>
                    <li><strong>Fusional languages</strong> (like Latin,
                        Russian, or Arabic) have morphemes that often fuse
                        multiple grammatical functions. For instance, a single
                        suffix might simultaneously mark case, number, and
                        gender.</li>
                    <li><strong>Agglutinative languages</strong> (like Turkish,
                        Finnish, or Japanese) string together morphemes in long
                        chains, with each morpheme typically representing one
                        grammatical function.</li>
                    <li><strong>Polysynthetic languages</strong> (like many
                        indigenous languages of the Americas, including
                        Inuktitut and Mohawk) can pack what would be an entire
                        sentence in English into a single complex word.</li>
                </ul>

                <p>The languages we're studying in this project—Rarámuri and
                    Shipibo-Konibo—fall toward the agglutinative and
                    polysynthetic end of this spectrum, making morphological
                    segmentation both challenging and particularly important for
                    understanding their structure.</p>

                <p>So why is morphological segmentation important? For starters,
                    morphology is critical for understanding how words are
                    formed across languages. Different languages build words in
                    different ways, and morphological analysis helps us
                    understand these patterns. From a computational perspective,
                    a proper morphological analysis can help with tasks like
                    machine translation, information retrieval, and text
                    generation, especially for morphologically rich languages
                    where a single word can encode what might be an entire
                    phrase in English.<label for="sn-1"
                        class="margin-toggle sidenote-number"></label><input
                        type="checkbox" id="sn-1" class="margin-toggle" /><span
                        class="sidenote">Bender, Emily M.
                        "Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Morphology and Syntax."
                        Morgan & Claypool Publishers, 2013.</span></p>

                <p>The practical applications of accurate morphological
                    segmentation are particularly salient for low-resource
                    languages. When working with limited data, capturing the
                    grammatical regularities encoded in morphology can
                    dramatically improve model performance on downstream tasks.
                    For instance, in machine translation, recognizing that
                    Turkish <em>evlerimden</em> decomposes into
                    <em>ev-ler-im-den</em> ("from my houses") allows a model to
                    generalize across all words with similar affixes, rather
                    than treating each word form as completely
                    independent.<label for="sn-mt-morph"
                        class="margin-toggle sidenote-number"></label><input
                        type="checkbox" id="sn-mt-morph"
                        class="margin-toggle" /><span class="sidenote">Ataman,
                        Duygu, et al.
                        "Unsupervised Neural Machine Translation with Unsupervised Morphological Segmentation."
                        EMNLP, 2019.</span></p>

                <p>In this project, I aimed to develop tokenizers that segment
                    text from two indigenous languages into morphemes, trying to
                    match gold standard segmentations created by linguists. The
                    languages are:</p>

                <ul>
                    <li><strong>Rarámuri (Tahumara) [tar]:</strong> an
                        indigenous language from Northern Mexico in the
                        Uto-Aztecan family. It has approximately 70,000 speakers
                        primarily in the state of Chihuahua. Rarámuri features
                        complex verbal morphology with numerous suffixes that
                        can be stacked to create intricate meanings.<label
                            for="sn-raramuri"
                            class="margin-toggle sidenote-number"></label><input
                            type="checkbox" id="sn-raramuri"
                            class="margin-toggle" /><span
                            class="sidenote">Caballero, Gabriela.
                            "Choguita Rarámuri (Tarahumara) Phonology and Morphology."
                            University of California, Berkeley,
                            2008.</span></li>
                    <li><strong>Shipibo-Konibo [shp]:</strong> a Panoan language
                        from the Peruvian Amazon with around 30,000 speakers.
                        It's characterized by its agglutinative structure and
                        extensive suffix system. The language has grammatical
                        features like ergative-absolutive alignment and a rich
                        system of evidential markers (indicating the source of
                        information).<label for="sn-shipibo"
                            class="margin-toggle sidenote-number"></label><input
                            type="checkbox" id="sn-shipibo"
                            class="margin-toggle" /><span
                            class="sidenote">Valenzuela, Pilar M.
                            "Transitivity in Shipibo-Konibo Grammar." University
                            of Oregon, 2003.</span></li>
                </ul>

                <p>These languages are particularly interesting for
                    morphological segmentation because they exhibit rich
                    morphological structures that differ significantly from
                    better-studied languages like English. Working with these
                    low-resource languages also presents an opportunity to
                    develop methods that could be valuable for the many other
                    understudied languages around the world.</p>
            </section>

            <section>
                <h2>The Dataset and Task</h2>

                <p>The dataset consists of words and their morphological
                    segmentations for both languages. For example:</p>

                <pre>
# Shipibo-Konibo examples
Source: yoyoaibata     Target: yoyoa ibat a
Source: kotsatax       Target: kotsat ax
Source: bokasai        Target: bo kas ai

# Rarámuri examples  
Source: páa            Target: páa
Source: pochítisi      Target: pochí ti si
Source: konári         Target: ko nári</pre>

                <p>My task was to develop a system that, given an unsegmented
                    word, produces a segmentation that matches the linguist's
                    gold segmentation as closely as possible. The evaluation
                    metric is precision/recall/F1 on a "bag of tokens" (rather
                    than exact segmentation boundaries) due to allomorphy in the
                    data.</p>

                <p>Speaking of allomorphy — that was a challenging aspect of
                    this dataset. Allomorphy is when a morpheme has different
                    forms (allomorphs) depending on its context. For example, in
                    English, the plural morpheme has allomorphs like "-s"
                    (cats), "-es" (boxes), and "-en" (oxen). In our dataset,
                    some morphemes don't concatenate perfectly, meaning there
                    can be additional characters at morpheme boundaries.<label
                        for="sn-2"
                        class="margin-toggle sidenote-number"></label><input
                        type="checkbox" id="sn-2" class="margin-toggle" /><span
                        class="sidenote">Haspelmath, Martin, and Andrea D. Sims.
                        "Understanding Morphology." Routledge, 2013.</span></p>

                <p>To deal with this, I implemented a preprocessing step that
                    "cleans" target segmentations using Levenshtein distance to
                    align segments with the source word. This ensures that our
                    model isn't penalized for segmentation errors due to
                    allomorphy.</p>

                <pre>
def clean_target_segmentation(src, tgt_line):
    tgt_segs = tgt_line.split()
    k = len(tgt_segs)
    n = len(src)
    dp = [[float('inf')] * (k+1) for _ in range(n+1)]
    bp = [[-1] * (k+1) for _ in range(n+1)]
    dp[0][0] = 0
    for i in range(n+1):
        for j in range(k):
            if dp[i][j] < float('inf'):
                for l in range(1, n - i + 1):
                    segment = src[i:i+l]
                    cost = levenshtein_distance(segment, tgt_segs[j])
                    if dp[i][j] + cost < dp[i+l][j+1]:
                        dp[i+l][j+1] = dp[i][j] + cost
                        bp[i+l][j+1] = l
    # Backtracking to get the best alignment
    if dp[n][k] == float('inf'):
        return tgt_segs
    segments = []
    i, j = n, k
    while j > 0:
        l = bp[i][j]
        segments.append(src[i-l:i])
        i -= l
        j -= 1
    return list(reversed(segments))</pre>

                <p>The baselines provided for comparison were:</p>

                <table>
                    <tr>
                        <th>Method</th>
                        <th>Shipibo-Konibo (F1)</th>
                        <th>Rarámuri (F1)</th>
                    </tr>
                    <tr>
                        <td>Morfessor</td>
                        <td>0.3207</td>
                        <td>0.2959</td>
                    </tr>
                    <tr>
                        <td>BPE</td>
                        <td>0.2682</td>
                        <td>0.4213</td>
                    </tr>
                    <tr>
                        <td>Unigram</td>
                        <td>0.2616</td>
                        <td>0.3369</td>
                    </tr>
                </table>

                <p>My goal was to beat these baselines with my own approach.</p>
            </section>

            <section>
                <h2>First Attempt: Information-Theoretic Approach</h2>

                <p>I started with an information-theoretic approach based on
                    conditional entropy. The intuition is that morpheme
                    boundaries often correspond to points of high uncertainty or
                    "surprisal" in character sequences. When you're reading a
                    word character by character, you'll be more surprised by the
                    first character of a new morpheme than by a character that
                    continues the current morpheme. This approach is
                    conceptually related to methods like BPE, but instead of
                    using frequency to identify units, we focus on identifying
                    boundaries based on predictability.</p>

                <p>This is actually a deeply principled approach. Morphemes are
                    by definition meaningful units, so they should have high
                    internal coherence (low surprisal within a morpheme) and
                    lower coherence across boundaries (high surprisal at
                    morpheme boundaries).<label for="sn-3"
                        class="margin-toggle sidenote-number"></label><input
                        type="checkbox" id="sn-3" class="margin-toggle" /><span
                        class="sidenote">Goldsmith, John.
                        "Unsupervised Learning of the Morphology of a Natural Language."
                        Computational Linguistics, 2001.</span></p>

                <p>This approach aligns with broader information-theoretic
                    principles in linguistics. Harris (1955) proposed that
                    morpheme boundaries occur at points of
                    unpredictability—where the number of possible next phonemes
                    suddenly increases. Similarly, the Minimum Description
                    Length (MDL) principle suggests that the most compact
                    representation of linguistic data should balance the size of
                    the grammar against the size of the data encoded by that
                    grammar.<label for="sn-mdl"
                        class="margin-toggle sidenote-number"></label><input
                        type="checkbox" id="sn-mdl"
                        class="margin-toggle" /><span class="sidenote">Rissanen,
                        Jorma. "Modeling by Shortest Data Description."
                        Automatica, 1978. The MDL principle has been applied to
                        morphological induction by several researchers,
                        including Goldsmith's Linguistica project.</span></p>

                <p>Mathematically, I calculated the surprisal of a character
                    given its prefix using:</p>

                <p>\[
                    S = -\log P(x \mid \text{prefix})
                    \]</p>

                <p>where the probability is estimated using an n-gram model
                    built from the training corpus. When this surprisal exceeds
                    a preset threshold, I place a morpheme boundary. The key
                    insight here is that the transition between morphemes should
                    be less predictable than transitions within morphemes,
                    resulting in higher surprisal values at boundaries.</p>

                <pre>
class NGramModel:
    # character-level n-gram model with conditional entropy segmentation
    
    def __init__(self, smoothing_constant=1.0, surprisal_threshold=3.0):
        self.smoothing_constant = smoothing_constant
        self.ngram_counts = defaultdict(lambda: defaultdict(int))
        self.vocabulary = set()
        self.surprisal_threshold = surprisal_threshold

    def fit(self, words):
        for word in words:
            word += '<END>'
            self.vocabulary.update(word)
            for i in range(len(word)):
                for j in range(1, i + 2):
                    prefix = word[i - j + 1:i]
                    char = word[i]
                    self.ngram_counts[prefix][char] += 1

    def next_letter_surprisal(self, prefix, char):
        count = self.ngram_counts[prefix][char]
        total = sum(self.ngram_counts[prefix].values())
        vocab_size = len(self.vocabulary)
        
        # surprisal = -log P(char | prefix)
        probability = (count + self.smoothing_constant) / (total + self.smoothing_constant * vocab_size)
        
        return -math.log(probability)

    def segment_word(self, word):
        segments = []
        start = 0
        for i in range(1, len(word)):
            prefix = word[start:i]
            next_char = word[i]
            surprisal = self.next_letter_surprisal(prefix, next_char)
            if surprisal > self.surprisal_threshold:
                segments.append(word[start:i])
                start = i
        segments.append(word[start:])
        return " ".join(segments)</pre>

                <p>I experimented with different surprisal thresholds to find
                    the optimal value for each language. Here's how the F1 score
                    varied with the threshold:</p>

                <figure>
                    <img src="/api/placeholder/800/400"
                        alt="Morphological Segmentation F1 Score vs. Surprisal Threshold">
                    <figcaption>Figure 1: F1 score as a function of surprisal
                        threshold for both languages</figcaption>
                </figure>

                <p>After tuning, I found the best thresholds to be around 3.06
                    for Shipibo-Konibo (F1 = 0.3326) and 2.86 for Rarámuri (F1 =
                    0.2680). This barely beats the Morfessor baseline for
                    Shipibo-Konibo but falls short for Rarámuri.</p>

                <p>While elegant in theory, this approach has limitations. It
                    relies solely on character-level statistics and doesn't
                    leverage linguistic patterns or semantic information that
                    neural approaches might capture. Still, I think it's a good
                    starting point and helps us understand the task from first
                    principles.</p>
            </section>

            <section>
                <h2>Second Attempt: Transformer-Based Approach</h2>

                <p>Next, I decided to try a more modern approach with neural
                    networks. I framed morphological segmentation as a sequence
                    labeling task, where each character is labeled as either the
                    start of a morpheme (1) or a continuation (0), and
                    implemented this using a transformer architecture.<label
                        for="sn-transformer"
                        class="margin-toggle sidenote-number"></label><input
                        type="checkbox" id="sn-transformer"
                        class="margin-toggle" /><span class="sidenote">These
                        days, it seems like transformers are applied to every
                        NLP problem regardless of whether they're the most
                        appropriate solution. While effective, I wanted to
                        explore if there were more principled approaches for
                        this specific task.</span></p>

                <p>I implemented a character-level transformer model with
                    positional encoding:</p>

                <pre>
class MorphemeTransformer(nn.Module):
    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, dropout, max_len):
        super(MorphemeTransformer, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.pos_encoder = PositionalEncoding(embed_dim, dropout, max_len)
        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dropout=dropout)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        self.fc = nn.Linear(embed_dim, 2)  # output logits for binary decision at each character
        self.embed_dim = embed_dim

    def forward(self, src):
        # src has shape (batch, seq_len); we embed and add positional encodings
        x = self.embedding(src)           # (batch, seq_len, embed_dim)
        x = self.pos_encoder(x)           # (batch, seq_len, embed_dim)
        x = x.transpose(0, 1)             # transformer expects (seq_len, batch, embed_dim)
        x = self.transformer_encoder(x)   # (seq_len, batch, embed_dim)
        x = x.transpose(0, 1)             # back to (batch, seq_len, embed_dim)
        logits = self.fc(x)               # (batch, seq_len, 2)
        return logits</pre>

                <p>I trained the model using cross-entropy loss on the training
                    set and evaluated it on the development set. The results
                    were surprisingly good (or maybe not so surprising if you're
                    used to transformers crushing traditional approaches):</p>

                <p>Shipibo-Konibo dev F1 score: 0.7843<br>
                    Rarámuri dev F1 score: 0.8357</p>

                <p>After training on the combined train and dev sets, the model
                    achieved test F1 scores of:</p>

                <p>Shipibo-Konibo test F1 score: 0.6435<br>
                    Rarámuri test F1 score: 0.4892</p>

                <p>These results significantly outperform the baselines, but I
                    felt that while effective, the transformer approach offered
                    limited linguistic insight into why particular segmentations
                    were chosen. This motivated me to explore more interpretable
                    approaches that could better capture the linguistic
                    principles behind morphological segmentation.</p>
            </section>

            <section>
                <h2>Third Attempt: Autoencoder Approach</h2>

                <p>Next, I explored an autoencoder-based approach. The idea was
                    to learn a compressed representation of the input data where
                    the latent space captures morpheme boundaries. This is
                    somewhat more principled than a pure transformer approach
                    because it's loosely connected to the information-theoretic
                    view of morphology (that morphemes are compressed
                    representations of meaning).<label for="sn-4"
                        class="margin-toggle sidenote-number"></label><input
                        type="checkbox" id="sn-4" class="margin-toggle" /><span
                        class="sidenote">Cotterell, Ryan, et al.
                        "Morphological Segmentation Inside-Out." EMNLP, 2016.
                        The connection between information theory and morphology
                        has been explored by several researchers who suggest
                        that languages evolve to balance expressivity and
                        compressibility, with morphemes serving as optimally
                        compressed units of meaning.</span></p>

                <p>However, autoencoders have a continuous latent space, while
                    morpheme boundaries are discrete. To address this, I used
                    the Gumbel-Softmax trick, which allows sampling from a
                    categorical distribution in a way that's differentiable.
                    This technique provides a continuous approximation to
                    discrete sampling by adding Gumbel noise and using a softmax
                    function with a temperature parameter that controls the
                    sharpness of the distribution.<label for="sn-gumbel"
                        class="margin-toggle sidenote-number"></label><input
                        type="checkbox" id="sn-gumbel"
                        class="margin-toggle" /><span class="sidenote">Jang,
                        Eric, et al.
                        "Categorical Reparameterization with Gumbel-Softmax."
                        ICLR, 2017. The paper that introduced the Gumbel-Softmax
                        trick for differentiable sampling from discrete
                        distributions.</span></p>

                <figure>
                    <img src="/api/placeholder/800/400"
                        alt="Autoencoder Architecture with Gumbel-Softmax">
                    <figcaption>Figure 2: Autoencoder architecture with
                        Gumbel-Softmax for discrete latent
                        representation</figcaption>
                </figure>

                <p>My autoencoder model architecture consists of:</p>

                <ol>
                    <li>A character embedding layer that converts input indices
                        to dense vectors</li>
                    <li>An LSTM encoder that produces hidden states for each
                        character</li>
                    <li>A linear projection layer that maps these hidden states
                        to logits for binary decisions (morpheme boundary or
                        not)</li>
                    <li>A Gumbel-Softmax layer that converts these logits to
                        differentiable discrete samples</li>
                    <li>A decoder LSTM that takes both the embeddings and the
                        discrete latent variables to reconstruct the input</li>
                </ol>

                <p>The implementation looks like this:</p>

                <pre>
class MorphAutoencoder(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, pad_idx):
        super(MorphAutoencoder, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)
        self.encoder = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.fc_latent = nn.Linear(hidden_dim, 2)  # Binary decision: morpheme boundary or not
        self.decoder = nn.LSTM(embed_dim + 2, hidden_dim, batch_first=True)
        self.fc_output = nn.Linear(hidden_dim, vocab_size)

    def forward(self, src_indices, tau=1.0, hard=True):
        # Embed input characters
        emb = self.embedding(src_indices)
        
        # Encode sequence
        enc_out, (h_n, c_n) = self.encoder(emb)
        
        # Generate logits for morpheme boundaries
        latent_logits = self.fc_latent(enc_out)
        
        # Apply Gumbel-Softmax for discrete latent space
        # This allows for backpropagation through discrete samples
        latent = F.gumbel_softmax(latent_logits, tau=tau, hard=hard, dim=-1)
        
        # Concatenate embeddings with latent variables for decoding
        dec_input = torch.cat([emb, latent], dim=-1)
        
        # Decode and reconstruct input
        dec_out, _ = self.decoder(dec_input)
        output_logits = self.fc_output(dec_out)
        
        return output_logits, latent_logits, latent</pre>

                <p>The Gumbel-Softmax trick works by adding Gumbel noise to the
                    logits and then applying a softmax function:</p>

                <p>\[
                    y_i = \frac{\exp((\log(\pi_i) + g_i) / \tau)}{\sum_{j=1}^{k}
                    \exp((\log(\pi_j) + g_j) / \tau)}
                    \]</p>

                <p>where \(g_i\) are samples from a Gumbel(0, 1) distribution,
                    \(\pi_i\) are the unnormalized probabilities (logits), and
                    \(\tau\) is the temperature parameter. As \(\tau\)
                    approaches 0, the samples become closer to one-hot vectors
                    (truly discrete), while higher values of \(\tau\) make the
                    distribution more uniform and the gradients smoother.</p>

                <p>I trained with two loss components: a reconstruction loss to
                    ensure the autoencoder reconstructs the input properly, and
                    a segmentation loss that encourages the latent
                    representation to match the true morpheme boundaries:</p>

                <pre>
# reconstruction loss
recon_loss = F.cross_entropy(output_logits.view(-1, output_logits.size(-1)),
                           src.view(-1), ignore_index=0)

# segmentation loss
seg_loss = F.cross_entropy(latent_logits.view(-1, 2), labels.view(-1))

loss = lambda_recon * recon_loss + lambda_seg * seg_loss</pre>

                <p>The balancing of these two loss components is crucial. If we
                    emphasize reconstruction too much, the model might learn to
                    copy the input perfectly without identifying meaningful
                    morpheme boundaries. Conversely, if we emphasize
                    segmentation too much, the reconstruction quality might
                    suffer, indicating that the latent representations aren't
                    capturing the full character-level information needed for
                    morphological analysis.</p>

                <p>I used Optuna, a hyperparameter optimization framework, to
                    tune the relative weights of these losses (lambda_seg and
                    lambda_recon) and found optimal values of lambda_seg=0.559
                    and lambda_recon=3.542 after 10 trials. The optimization
                    objective was to maximize the F1 score on the development
                    set.</p>

                <pre>
def objective(trial):
    lambda_seg = trial.suggest_float("lambda_seg", 0.5, 2.0)
    lambda_recon = trial.suggest_float("lambda_recon", 1.0, 5.0)
    
    lang = 'tar'
    char2idx_lang = create_char2idx(data, lang)
    idx2char_lang = create_idx2char(char2idx_lang)
    vocab_size = len(char2idx_lang)
    pad_idx = char2idx_lang['<PAD>']
    
    model = MorphAutoencoder(vocab_size, embed_dim=embed_dim, hidden_dim=hidden_dim, pad_idx=pad_idx).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)
    train_loader = train_loaders[lang]
    train_autoencoder(model, train_loader, optimizer, num_epochs=num_epochs, device=device,
                      tau=1.0, lambda_seg=lambda_seg, lambda_recon=lambda_recon)
    
    dev_loader = dev_loaders[lang]
    avg_loss, f1 = evaluate_autoencoder(model, dev_loader, device, idx2char_lang,
                                        tau=1.0, lambda_seg=lambda_seg, lambda_recon=lambda_recon)
    print(f"Trial: lambda_seg={lambda_seg:.3f}, lambda_recon={lambda_recon:.3f}, F1={f1:.4f}")
    return -f1

study = optuna.create_study(direction="minimize")
study.optimize(objective, n_trials=10)</pre>

                <p>The results on the development set after training with the
                    optimized hyperparameters were:</p>

                <p>Shipibo-Konibo dev F1 score: 0.6957<br>
                    Rarámuri dev F1 score: 0.7346</p>

                <p>And on the test set:</p>

                <p>Shipibo-Konibo test F1 score: 0.7380<br>
                    Rarámuri test F1 score: 0.5024</p>

                <p>This approach outperformed the transformer for Shipibo-Konibo
                    but was slightly worse for Rarámuri. Still, I found this
                    approach more satisfying because it attempted to model the
                    actual structure of morphology rather than just learning
                    statistical patterns, representing a significant step
                    forward for interpretability and linguistic grounding.</p>
            </section>

            <section>
                <h2>Final Approach: Neural Scoring for Segmentation</h2>

                <p>After mulling over the results for a few days, I had a
                    realization. What if we could combine the principled
                    information-theoretic approach with the power of neural
                    networks? This led me to explore differentiable programming,
                    where you take a traditionally non-differentiable algorithm
                    and reformulate it in a continuous way so that the entire
                    process can be optimized using gradient descent.<label
                        for="sn-5"
                        class="margin-toggle sidenote-number"></label><input
                        type="checkbox" id="sn-5" class="margin-toggle" /><span
                        class="sidenote">Mensch, Arthur, and Mathieu Blondel.
                        "Differentiable Dynamic Programming for Structured Prediction and Attention."
                        ICML, 2018. This approach has been gaining traction in
                        the ML community as a way to incorporate algorithmic
                        priors while maintaining end-to-end differentiability,
                        effectively getting the best of both worlds between
                        traditional algorithms and neural approaches.</span></p>

                <p>This approach draws inspiration from structured prediction
                    methods in machine learning, which aim to predict structured
                    objects (like sequences or trees) rather than simple class
                    labels. The key insight is that we want to score complete
                    segmentations of a word, not just make independent decisions
                    at each character position. This is important because
                    morphological segmentation decisions are inherently
                    interdependent — the decision to place a boundary at one
                    position affects the plausibility of boundaries at other
                    positions.</p>

                <p>The plan was to use a neural scoring function to assign a
                    quality score to every possible candidate segment in a word.
                    Concretely, I implemented the following steps:</p>

                <ol>
                    <li><strong>Encoding:</strong> Run a character-level LSTM
                        over the input word, obtaining hidden representations
                        \(h_1, h_2, \dots, h_n\) at each position.</li>
                    <li><strong>Scoring Segments:</strong> For any candidate
                        segment spanning positions \(i\) to \(j\), compute a
                        score \(s(i,j) = \text{MLP}(h_i \oplus h_{j-1})\), where
                        \(\oplus\) denotes vector concatenation. This score
                        reflects how plausible it is that the substring forms a
                        valid morpheme.</li>
                    <li><strong>Global Segmentation Score:</strong> Use a
                        differentiable dynamic programming algorithm to compute
                        the global score of the segmentation and the
                        corresponding partition function \(Z\).</li>
                    <li><strong>Loss Function:</strong> Define the loss as
                        \(\mathcal{L} = \log Z - s_{\text{gold}}\), which
                        encourages the model to assign a higher global score to
                        the correct segmentation relative to all possible
                        segmentations.</li>
                </ol>

                <p>The dynamic programming algorithm is particularly
                    interesting. It efficiently computes the partition function
                    \(Z\), which is the sum of scores over all possible
                    segmentations:</p>

                <p>\[
                    Z = \sum_{S} \exp(s(S))
                    \]</p>

                <p>where \(s(S)\) is the score of segmentation \(S\). Taking the
                    logarithm, we get:</p>

                <p>\[
                    \log Z = \log\left(\sum_{S} \exp(s(S))\right)
                    \]</p>

                <p>This can be computed efficiently using dynamic programming
                    with the following recurrence relation:</p>

                <p>\[
                    \text{dp}[i] = \log\sum_{j=0}^{i-1} \exp(\text{dp}[j] +
                    s(j,i))
                    \]</p>

                <p>where \(\text{dp}[i]\) represents the log-partition function
                    for the first \(i\) characters of the word.</p>

                <p>Here's the implementation of the Segmenter model:</p>

                <pre>
class Segmenter(nn.Module):
    def __init__(self, vocab_size, emb_dim, hidden_dim, max_seg_len, char2idx, idx2char):
        super().__init__()
        self.pad_idx = char2idx['<PAD>']
        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=self.pad_idx)
        self.lstm = nn.LSTM(emb_dim, hidden_dim, bidirectional=True, batch_first=True)
        # scoring MLP
        self.mlp = nn.Sequential(nn.Linear(4*hidden_dim, 128),
                               nn.ReLU(), 
                               nn.Linear(128, 1))
        self.idx2char = idx2char

    # Score a segment from position i to j
    def segment_score(self, h, i, j):
        seg_rep = torch.cat([h[i], h[j-1]], dim=-1)
        return self.mlp(seg_rep).squeeze()
    
    # Compute loss using dynamic programming
    def dp_loss(self, h, gold_boundaries):
        L = h.size(0)
        dp = [None]*(L+1)
        dp[0] = torch.tensor(0.0, device=h.device)
        
        # Forward pass: compute log-partition function
        for i in range(1, L+1):
            candidates = []
            for j in range(0, i):
                score = self.segment_score(h, j, i)
                candidates.append(dp[j] + score)
                
            dp[i] = torch.logsumexp(torch.stack(candidates), dim=0)
        
        # Compute score of gold segmentation
        logZ = dp[L]
        gold_score = 0.0
        gold_boundaries_full = gold_boundaries[:]
        
        if gold_boundaries_full[-1] != L:
            gold_boundaries_full.append(L)
        
        for idx in range(1, len(gold_boundaries_full)):
            j = gold_boundaries_full[idx-1]
            i = gold_boundaries_full[idx]
            gold_score = gold_score + self.segment_score(h, j, i)
        
        # Return negative log-likelihood: log(Z) - score(gold)
        return logZ - gold_score

    # Viterbi decoding to find best segmentation
    def viterbi_decode(self, h):
        L = h.size(0)
        dp = [None]*(L+1)           # dp[i] stores the best score for the first i characters
        bp = [None]*(L+1)           # bp[i] stores the best boundary before the i-th character
        dp[0] = 0.0
        
        # dp[i] = max(dp[j] + score(j, i)) for 0 <= j < i
        for i in range(1, L+1):
            best_score = -float('inf')
            best_j = None

            for j in range(0, i):
                score = dp[j] + self.segment_score(h, j, i).item()
                if score > best_score:
                    best_score = score
                    best_j = j
            dp[i] = best_score
            bp[i] = best_j
        
        # Backtrack to recover the best segmentation
        boundaries = []
        i = L
        while i > 0:
            boundaries.append(bp[i])
            i = bp[i]
        
        boundaries = boundaries[::-1]
        boundaries.append(L)
        
        return boundaries

    # Forward pass
    def forward(self, src, labels=None):
        emb = self.emb(src)
        out, _ = self.lstm(emb)
        
        if labels is not None:
            loss = 0.0    
            for b in range(src.size(0)):
                l = (src[b] != self.pad_idx).sum().item()
                h = out[b, :l, :]
                gold_b = self.get_gold_boundaries(labels[b, :l])
                loss = loss + self.dp_loss(h, gold_b)
            return loss / src.size(0)
        else:
            preds = []
            for b in range(src.size(0)):
                l = (src[b] != self.pad_idx).sum().item()
                h = out[b, :l, :]
                preds.append(self.decode_word(h, src[b]))
            return preds</pre>

                <p>This approach combines the best aspects of traditional
                    dynamic programming algorithms (which ensure global
                    consistency in segmentation decisions) with the
                    representational power of neural networks (which can learn
                    complex character-level patterns). The differentiable nature
                    of the entire pipeline allows end-to-end training via
                    gradient descent.</p>

                <figure>
                    <img src="/api/placeholder/800/400"
                        alt="Neural Segmentation with Dynamic Programming">
                    <figcaption>Figure 3: Neural segmentation with dynamic
                        programming. The model computes segment scores using a
                        neural network, then uses dynamic programming to find
                        globally optimal segmentations.</figcaption>
                </figure>

                <p>I trained this model for 25 epochs and achieved:</p>

                <p>Shipibo-Konibo dev F1 score: 0.7854<br>
                    Rarámuri dev F1 score: 0.7493</p>

                <p>And on the test set:</p>

                <p>Shipibo-Konibo test F1 score: 0.8126<br>
                    Rarámuri test F1 score: 0.6018</p>

                <p>These are the best results yet! The combination of linguistic
                    theory (via structured prediction) and modern neural methods
                    yielded superior performance to either approach alone,
                    demonstrating the value of integrating theoretical
                    linguistic knowledge with data-driven techniques.</p>
            </section>

            <section>
                <h2>Conclusion and Final Results</h2>

                <p>Let's summarize the results across all my approaches:</p>

                <table>
                    <tr>
                        <th>Method</th>
                        <th>Shipibo-Konibo (F1)</th>
                        <th>Rarámuri (F1)</th>
                    </tr>
                    <tr>
                        <td>Morfessor (baseline)</td>
                        <td>0.3207</td>
                        <td>0.2959</td>
                    </tr>
                    <tr>
                        <td>BPE (baseline)</td>
                        <td>0.2682</td>
                        <td>0.4213</td>
                    </tr>
                    <tr>
                        <td>Unigram (baseline)</td>
                        <td>0.2616</td>
                        <td>0.3369</td>
                    </tr>
                    <tr>
                        <td>Conditional Entropy</td>
                        <td>0.3326</td>
                        <td>0.2680</td>
                    </tr>
                    <tr>
                        <td>Transformer</td>
                        <td>0.6435</td>
                        <td>0.4892</td>
                    </tr>
                    <tr>
                        <td>Autoencoder</td>
                        <td>0.7380</td>
                        <td>0.5024</td>
                    </tr>
                    <tr>
                        <td>Neural Scoring</td>
                        <td>0.8126</td>
                        <td>0.6018</td>
                    </tr>
                </table>

                <figure>
                    <img src="/api/placeholder/800/500"
                        alt="Comparison of F1 scores across different approaches">
                    <figcaption>Figure 4: Comparison of F1 scores across
                        different morphological segmentation approaches for both
                        languages. The neural scoring approach with dynamic
                        programming consistently outperforms other
                        methods.</figcaption>
                </figure>

                <p>The progression from traditional methods to neural approaches
                    and finally to a hybrid approach shows a clear trend of
                    improvement. The neural scoring method, which combines the
                    structural insights of dynamic programming with the
                    representational power of neural networks, achieved the best
                    results by a significant margin.</p>

                <p>It's worth reflecting on why the neural scoring approach
                    worked so well. I believe there are several key factors:</p>

                <ol>
                    <li><strong>Global consistency:</strong> Unlike the
                        transformer and autoencoder approaches, which make
                        decisions about morpheme boundaries at each position
                        independently, the neural scoring approach enforces
                        global consistency by considering complete
                        segmentations.</li>
                    <li><strong>Linguistic inductive bias:</strong> The dynamic
                        programming component encodes the linguistic intuition
                        that words consist of a sequence of morphemes, providing
                        a strong inductive bias that guides the learning
                        process.</li>
                    <li><strong>Representational power:</strong> The
                        bidirectional LSTM encoder captures complex contextual
                        patterns in character sequences, allowing the model to
                        learn language-specific regularities that signal
                        morpheme boundaries.</li>
                    <li><strong>End-to-end optimization:</strong> The
                        differentiable nature of the entire pipeline allows
                        gradient-based optimization of the neural network
                        parameters to directly maximize the probability of
                        correct segmentations.</li>
                </ol>

                <p>Interestingly, the relative performance of different
                    approaches varied between the two languages. The neural
                    scoring approach showed a more dramatic improvement for
                    Shipibo-Konibo than for Rarámuri, suggesting that the
                    morphological patterns in these languages may differ in ways
                    that make them more or less amenable to different
                    computational approaches.<label for="sn-lang-diff"
                        class="margin-toggle sidenote-number"></label><input
                        type="checkbox" id="sn-lang-diff"
                        class="margin-toggle" /><span class="sidenote">This
                        observation aligns with linguistic descriptions:
                        Shipibo-Konibo has been characterized as having more
                        regular agglutinative morphology, while Rarámuri
                        exhibits more complex morphophonological processes that
                        can obscure morpheme boundaries. These language-specific
                        properties likely impact the effectiveness of different
                        computational approaches.</span></p>

                <p>This project taught me several valuable lessons. First, while
                    neural networks are powerful, they work best when combined
                    with principled linguistic insights. Second, structured
                    prediction (via dynamic programming) provides a way to
                    ensure global consistency in segmentation decisions. And
                    finally, sometimes the best approach is not the most complex
                    one, but rather the one that best captures the underlying
                    structure of the problem.</p>

                <p>For future work, there are several promising directions to
                    explore:</p>

                <ol>
                    <li><strong>Incorporating morphophonological rules:</strong>
                        Many languages have specific phonological processes that
                        occur at morpheme boundaries, such as vowel harmony,
                        consonant gradation, or assimilation. Explicitly
                        modeling these processes could further improve
                        segmentation accuracy.<label for="sn-morphophono"
                            class="margin-toggle sidenote-number"></label><input
                            type="checkbox" id="sn-morphophono"
                            class="margin-toggle" /><span
                            class="sidenote">Cotterell, Ryan, et al.
                            "Joint Morphological and Syntactic Analysis for Richly Inflected Languages."
                            TACL, 2015.</span></li>
                    <li><strong>Multilingual models:</strong> Training a single
                        model on multiple related languages could help capture
                        shared morphological patterns and improve performance,
                        particularly for low-resource languages.<label
                            for="sn-multilingual"
                            class="margin-toggle sidenote-number"></label><input
                            type="checkbox" id="sn-multilingual"
                            class="margin-toggle" /><span
                            class="sidenote">McCarthy, Arya D., et al.
                            "Marrying Universal Dependencies and Universal Morphology."
                            Proceedings of the Second Workshop on Universal
                            Dependencies, 2018.</span></li>
                    <li><strong>Semi-supervised learning:</strong> Incorporating
                        unlabeled data through techniques like self-training or
                        co-training could improve performance without requiring
                        additional annotated examples.<label
                            for="sn-semisupervised"
                            class="margin-toggle sidenote-number"></label><input
                            type="checkbox" id="sn-semisupervised"
                            class="margin-toggle" /><span
                            class="sidenote">Ruder, Sebastian, and Barbara
                            Plank.
                            "Strong Baselines for Neural Semi-supervised Learning under Domain Shift."
                            ACL, 2018.</span></li>
                    <li><strong>Hierarchical segmentation:</strong> Some
                        languages have complex morphological structures with
                        multiple levels (e.g., derivation followed by
                        inflection). Developing models that can capture this
                        hierarchical structure would be an interesting
                        challenge.<label for="sn-hierarchical"
                            class="margin-toggle sidenote-number"></label><input
                            type="checkbox" id="sn-hierarchical"
                            class="margin-toggle" /><span class="sidenote">Kann,
                            Katharina, et al.
                            "Neural Multi-step Reasoning for Question Answering on Semi-structured Tables."
                            EMNLP, 2020.</span></li>
                    <li><strong>Extension to other morphological types:</strong>
                        While the languages in this study exhibit primarily
                        concatenative morphology, many languages use
                        non-concatenative processes like infixation,
                        reduplication, or templatic morphology. Adapting our
                        approach to handle these cases would be a significant
                        contribution.<label for="sn-nonconcatenative"
                            class="margin-toggle sidenote-number"></label><input
                            type="checkbox" id="sn-nonconcatenative"
                            class="margin-toggle" /><span
                            class="sidenote">Kirov, Christo, and Ryan Cotterell.
                            "Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate."
                            TACL, 2018.</span></li>
                </ol>

                <p>Overall, I'm quite satisfied with the results of this
                    project. We significantly outperformed the baselines and
                    developed a principled approach that combines the best of
                    both traditional and neural methods. The success of the
                    neural scoring approach with dynamic programming reinforces
                    the value of incorporating linguistic knowledge into machine
                    learning models, especially for structured prediction tasks
                    like morphological segmentation.</p>

                <p>As NLP moves beyond English and other high-resource
                    languages, techniques like those explored in this project
                    will become increasingly important for developing language
                    technologies that work effectively across the full diversity
                    of human languages.</p>
            </section>
        </article>

        <script>
    document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body, {
        delimiters: [
          {left: "$$", right: "$$", display: true},
          {left: "\\[", right: "\\]", display: true},
          {left: "$", right: "$", display: false},
          {left: "\\(", right: "\\)", display: false}
        ]
      });
    });
  </script>
    </body>
</html>